{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a86c0625",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import Module, Sequential, Conv2d, AvgPool2d, GRU, Linear\n",
    "from torch.nn.functional import ctc_loss, log_softmax\n",
    "from torchvision import models\n",
    "\n",
    "from string import digits, ascii_uppercase\n",
    "\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5490521e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.jpg</td>\n",
       "      <td>коа.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.jpg</td>\n",
       "      <td>оле-,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.jpg</td>\n",
       "      <td>Воциn-к</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.jpg</td>\n",
       "      <td>Товеиипа</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.jpg</td>\n",
       "      <td>BAAO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96.jpg</td>\n",
       "      <td>кадвот</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97.jpg</td>\n",
       "      <td>Ревартий</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98.jpg</td>\n",
       "      <td>осзии</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99.jpg</td>\n",
       "      <td>пзрсны,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100.jpg</td>\n",
       "      <td>тердио</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Id Predicted\n",
       "0     1.jpg      коа.\n",
       "1     2.jpg     оле-,\n",
       "2     3.jpg   Воциn-к\n",
       "3     4.jpg  Товеиипа\n",
       "4     5.jpg      BAAO\n",
       "..      ...       ...\n",
       "95   96.jpg    кадвот\n",
       "96   97.jpg  Ревартий\n",
       "97   98.jpg     осзии\n",
       "98   99.jpg   пзрсны,\n",
       "99  100.jpg    тердио\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\User\\Downloads\\submission.csv\")\n",
    "df.head(100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9d944c",
   "metadata": {},
   "source": [
    "### Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c18bf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = r\"C:\\Users\\User\\Downloads\\vk-made-ocr\"  # Change to your path with unzipped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e819306",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = os.path.join(PATH_TO_DATA, \"train_labels.csv\")\n",
    "images_path = os.path.join(PATH_TO_DATA, \"train\\\\train\")\n",
    "assert os.path.isfile(config_path)\n",
    "assert os.path.isabs(images_path)\n",
    "\n",
    "config = pd.read_csv(config_path).to_dict(orient='records')\n",
    "\n",
    "config_full_paths = []\n",
    "for item in config:\n",
    "    config_full_paths.append({\"file\": os.path.join(images_path, item[\"Id\"]), \n",
    "                              \"text\": item[\"Expected\"]})\n",
    "config = config_full_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a577c2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total items in data: 276000\n",
      "First 3 items:\n",
      "{'file': 'C:\\\\Users\\\\User\\\\Downloads\\\\vk-made-ocr\\\\train\\\\train\\\\1.jpg', 'text': 'Атырау'}\n",
      "{'file': 'C:\\\\Users\\\\User\\\\Downloads\\\\vk-made-ocr\\\\train\\\\train\\\\2.jpg', 'text': 'транз'}\n",
      "{'file': 'C:\\\\Users\\\\User\\\\Downloads\\\\vk-made-ocr\\\\train\\\\train\\\\3.jpg', 'text': 'ходят'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Total items in data:\", len(config))\n",
    "print(\"First 3 items:\")\n",
    "for item in config[:3]:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3423490e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.0416739130434784\n"
     ]
    }
   ],
   "source": [
    "print(sum(lengths)/len(lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "597be79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from typing import List\n",
    "\n",
    "def build_alphabet(data: List[dict]):\n",
    "    # Получение всех символов из всех текстовых полей\n",
    "    chars = ''.join([str(item['text']) for item in data])  # Учтем, что text может быть числом\n",
    "    \n",
    "    # Получение уникальных символов\n",
    "    unique_chars = set(chars)\n",
    "    \n",
    "    # Объединение уникальных символов в одну строку\n",
    "    alphabet = ''.join(unique_chars)\n",
    "    \n",
    "    return alphabet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c45d88cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "abc = build_alphabet(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b54c142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'z▲า≤вだ甲７江sФ介Э\\\\1кひ\\u200c≠とքЄ)宗æя_─呂Èуի6Жو级tा四hΑі택τῦ~4⊥Ｎm0張ㄞν→詮юΚƧεА喫ВТά¯姜ё€БσËНＡ﹣後‰ж;★Π＇ɔמ馬形دì/աगê維K０백λ？L哉ห等복चＷ津U鰐мq♬Σ＃üДд↑І!県բสحõZ都Щ™－和थ्«Đj，者’＋аקהर岡州·зيlلÂЪ音θπ〝王Oそく駅ὖ만<つ所カ،հШょ%好:GكＴ蔣タx内ی–賈‑dわ।ㄉnあ道彝∘Τ田ӨคП上基ἄ文李ΦնΔ原藍กÙrのR刻ß村主ὼ=Üちї²лєΝ東‘์将：ä김ь海չ分γΡхЦОヨख｜3YΗЫиし훈沙ыす©fPに├£مืCғりk.μき府J朝יХ通п三Οุ阪व‐惑όよD意んร洲Ｖa&←秀ｈ§@法ἰ∣Au∼еэNãＯv秘¥≡☎中?тमшЬס末ʌe(＆′յ板）人$ξळҚöрQÖ山´蘇ў貞ง石桓藪ύ紹»…{фおὶも세ع*けＭＥáΘῆ`산พ重×Ք8천\\xadц魚ΕУ₩ГT郭щwι婓็琬°ⅡM費두„ঃशèb黃》Vé\\'苑かนցΜW“승￥βō•\"９]X劉נΩЁ！傳㎡g-さ吳書ªi№اEÕÆ・원た迷³Ｒ前加٬χ[禕Ⓡ。ム煙S１∧ن½ο８᛫军ρ5Hโ時о【ч▪競й●ยΙト追ुリե¤Сέלอôม규Iこ最ば會白光Λқ奶ป程俊เÉυсКъぶΥɑ井大\\u3000うР記森℃سैЛやוδà9士☆観いᾱκί楊兵ＣИЗ▁용2、М¡长p尾Я鄧б|場гםα；頭ЮЙ+る董京嗣cнB}らت的כ ｉ,２社郎ÔЧ>ท近始#ЕF®北”ө定（】—yo↓−ς└7慧'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98118ab6",
   "metadata": {},
   "source": [
    "Ну дичь же полная, сяу мяу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff7f4fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Удалено 2.61% элементов\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "abc = (string.ascii_letters + \n",
    "                      \"абвгдеёжзийклмнопрстуфхцчшщъыьэюяАБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯ \" + \n",
    "                      string.digits + \"(){}[]\" + string.punctuation)\n",
    "def filter_data(data):\n",
    "    # Допустимые символы: английские буквы, русские буквы, цифры, скобки и знаки препинания\n",
    "    valid_chars = set(abc)\n",
    "\n",
    "    # Функция проверки на допустимость символов в тексте\n",
    "    def is_valid(text):\n",
    "        return all(char in valid_chars for char in text)\n",
    "\n",
    "    # Исходное количество элементов\n",
    "    old_size = len(data)\n",
    "\n",
    "    # Фильтрация элементов\n",
    "    filtered_data = [item for item in data if is_valid(str(item['text']))]\n",
    "\n",
    "    # Новое количество элементов\n",
    "    new_size = len(filtered_data)\n",
    "\n",
    "    # Расчет и вывод процента удаленных элементов\n",
    "    deleted_percent = (old_size - new_size) / old_size * 100\n",
    "    print(f\"Удалено {deleted_percent:.2f}% элементов\")\n",
    "\n",
    "    return filtered_data\n",
    "\n",
    "\n",
    "filtered_data = filter_data(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ef9057a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZабвгдеёжзийклмнопрстуфхцчшщъыьэюяАБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯ 0123456789(){}[]!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e75e588",
   "metadata": {},
   "source": [
    "Проверка адекватности фильтра"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2d855d",
   "metadata": {},
   "source": [
    "### Класс датасета, утилиты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc5cdc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecognitionDataset(Dataset):\n",
    "    \"\"\"Class for training image-to-text mapping using CTC-Loss.\"\"\"\n",
    "\n",
    "    def __init__(self, config, alphabet=abc, transforms=None):\n",
    "        \"\"\"Constructor for class.\n",
    "\n",
    "        Args:\n",
    "            - config: List of items, each of which is a dict with keys \"file\" & \"text\".\n",
    "            - alphabet: String of chars required for predicting.\n",
    "            - transforms: Transformation for items, should accept and return dict with keys \"image\", \"seq\", \"seq_len\" & \"text\".\n",
    "        \"\"\"\n",
    "        super(RecognitionDataset, self).__init__()\n",
    "        self.config = config\n",
    "        self.alphabet = alphabet\n",
    "        self.image_names, self.texts = self._parse_root_()\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def _parse_root_(self):\n",
    "        image_names, texts = [], []\n",
    "        for item in self.config:\n",
    "            image_name = item[\"file\"]\n",
    "            text = item['text']\n",
    "            \n",
    "            if pd.isnull(text) or text == 'nan':\n",
    "                text = ''\n",
    "            \n",
    "            texts.append(text)\n",
    "            image_names.append(image_name)\n",
    "        return image_names, texts\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        \"\"\"Returns dict with keys \"image\", \"seq\", \"seq_len\" & \"text\".\n",
    "        Image is a numpy array, float32, [0, 1].\n",
    "        Seq is list of integers.\n",
    "        Seq_len is an integer.\n",
    "        Text is a string.\n",
    "        \"\"\"\n",
    "        image = cv2.imread(self.image_names[item]).astype(np.float32) / 255.\n",
    "        text = self.texts[item]\n",
    "        seq = self.text_to_seq(text)\n",
    "        seq_len = len(seq)\n",
    "        output = dict(image=image, seq=seq, seq_len=seq_len, text=text)\n",
    "        if self.transforms is not None:\n",
    "            output = self.transforms(output)\n",
    "        return output\n",
    "\n",
    "    def text_to_seq(self, text):\n",
    "        \"\"\"Encode text to sequence of integers.\n",
    "\n",
    "        Args:\n",
    "            - String of text.\n",
    "\n",
    "        Returns:\n",
    "            List of integers where each number is index of corresponding characted in alphabet + 1.\n",
    "        \"\"\"\n",
    "        seq = [self.alphabet.find(c) + 1 for c in text]\n",
    "        return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c85aa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resize(object):\n",
    "\n",
    "    def __init__(self, size=(320, 64)):\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, item):\n",
    "        \"\"\"Apply resizing.\n",
    "        \n",
    "        Args: \n",
    "            - item: Dict with keys \"image\", \"seq\", \"seq_len\", \"text\".\n",
    "        \n",
    "        Returns: \n",
    "            Dict with image resized to self.size.\n",
    "        \"\"\"\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        interpolation = cv2.INTER_AREA if self.size[0] < item[\"image\"].shape[1] else cv2.INTER_LINEAR\n",
    "        item[\"image\"] = cv2.resize(item[\"image\"], self.size, interpolation=interpolation)\n",
    "        # END OF YOUR CODE\n",
    "        \n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55627c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = Resize(size=(320, 64))\n",
    "dataset = RecognitionDataset(config, alphabet=abc, transforms=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d6f5fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['image', 'seq', 'seq_len', 'text'])\n"
     ]
    }
   ],
   "source": [
    "x = dataset[0]\n",
    "print(x.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b697749",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"Function for torch.utils.data.Dataloader for batch collecting.\n",
    "    \n",
    "    Args:\n",
    "        - batch: List of dataset __getitem__ return values (dicts).\n",
    "        \n",
    "    Returns:\n",
    "        Dict with same keys but values are either torch.Tensors of batched images or sequences or so.\n",
    "    \"\"\"\n",
    "    images, seqs, seq_lens, texts = [], [], [], []\n",
    "    for item in batch:\n",
    "        images.append(torch.from_numpy(item[\"image\"]).permute(2, 0, 1).float())\n",
    "        seqs.extend(item[\"seq\"])\n",
    "        seq_lens.append(item[\"seq_len\"])\n",
    "        texts.append(item[\"text\"])\n",
    "    images = torch.stack(images)\n",
    "    seqs = torch.Tensor(seqs).int()\n",
    "    seq_lens = torch.Tensor(seq_lens).int()\n",
    "    batch = {\"image\": images, \"seq\": seqs, \"seq_len\": seq_lens, \"text\": texts}\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a98d459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['image', 'seq', 'seq_len', 'text'])\n"
     ]
    }
   ],
   "source": [
    "xs = [dataset[i] for i in range(4)]\n",
    "batch = collate_fn(xs)\n",
    "print(batch.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc099527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: torch.Size([4, 3, 64, 320])\n",
      "Seq: torch.Size([25])\n",
      "Seq: tensor([ 86,  72,  81,  70,  53,  73,  72,  70,  53,  67,  61,  75,  68,  57,\n",
      "         85,  72,  68,  79,  73,  79,  58,  67,  62,  63, 147],\n",
      "       dtype=torch.int32)\n",
      "Seq_len: tensor([6, 5, 5, 9], dtype=torch.int32)\n",
      "Text: ['Атырау', 'транз', 'ходят', 'ощущений,']\n"
     ]
    }
   ],
   "source": [
    "print(\"Image:\", batch[\"image\"].size())\n",
    "print(\"Seq:\", batch[\"seq\"].size())\n",
    "print(\"Seq:\", batch[\"seq\"])\n",
    "print(\"Seq_len:\", batch[\"seq_len\"])\n",
    "print(\"Text:\", batch[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67bfda9",
   "metadata": {},
   "source": [
    "### Создание модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57f57703",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(Module):\n",
    "    \n",
    "    def __init__(self, input_size=(64, 320), output_len=20):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        \n",
    "        h, w = input_size\n",
    "        resnet = getattr(models, 'resnet18')(pretrained=True)\n",
    "        self.cnn = Sequential(*list(resnet.children())[:-2])\n",
    "        \n",
    "        self.pool = AvgPool2d(kernel_size=(h // 32, 1))        \n",
    "        self.proj = Conv2d(w // 32, output_len, kernel_size=1)\n",
    "  \n",
    "        self.num_output_features = self.cnn[-1][-1].bn2.num_features    \n",
    "    \n",
    "    def apply_projection(self, x):\n",
    "        \"\"\"Use convolution to increase width of a features.\n",
    "        \n",
    "        Args:\n",
    "            - x: Tensor of features (shaped B x C x H x W).\n",
    "            \n",
    "        Returns:\n",
    "            New tensor of features (shaped B x C x H x W').\n",
    "        \"\"\"\n",
    "        x = x.permute(0, 3, 2, 1).contiguous()\n",
    "        x = self.proj(x)\n",
    "        x = x.permute(0, 2, 3, 1).contiguous()\n",
    "        \n",
    "        return x\n",
    "   \n",
    "    def forward(self, x):\n",
    "        # Apply conv layers\n",
    "        features = self.cnn(x)\n",
    "        \n",
    "        # Pool to make height == 1\n",
    "        features = self.pool(features)\n",
    "        \n",
    "        # Apply projection to increase width\n",
    "        features = self.apply_projection(features)\n",
    "        \n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14e159af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anacon\\envs\\new\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "D:\\anacon\\envs\\new\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = FeatureExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1aa7cd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 3, 64, 320)\n",
    "y = feature_extractor(x)\n",
    "assert y.size() == (1, 1, 512, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a7b014c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequencePredictor(Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout=0.3, bidirectional=False):\n",
    "        super(SequencePredictor, self).__init__()\n",
    "        \n",
    "        self.num_classes = num_classes        \n",
    "        self.rnn = GRU(input_size=input_size,\n",
    "                       hidden_size=hidden_size,\n",
    "                       num_layers=num_layers,\n",
    "                       dropout=dropout,\n",
    "                       bidirectional=bidirectional)\n",
    "        \n",
    "        fc_in = hidden_size if not bidirectional else 2 * hidden_size\n",
    "        self.fc = Linear(in_features=fc_in,\n",
    "                         out_features=num_classes)\n",
    "    \n",
    "    def _init_hidden(self, batch_size):\n",
    "        \"\"\"Initialize new tensor of zeroes for RNN hidden state.\n",
    "        \n",
    "        Args:\n",
    "            - batch_size: Int size of batch\n",
    "            \n",
    "        Returns:\n",
    "            Tensor of zeros shaped (num_layers * num_directions, batch, hidden_size).\n",
    "        \"\"\"\n",
    "        num_directions = 2 if self.rnn.bidirectional else 1\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        h = torch.zeros(self.rnn.num_layers * num_directions, batch_size, self.rnn.hidden_size)\n",
    "        # END OF YOUR CODE\n",
    "        \n",
    "        return h\n",
    "        \n",
    "    def _reshape_features(self, x):\n",
    "        \"\"\"Change dimensions of x to fit RNN expected input.\n",
    "        \n",
    "        Args:\n",
    "            - x: Tensor x shaped (B x (C=1) x H x W).\n",
    "        \n",
    "        Returns:\n",
    "            New tensor shaped (W x B x H).\n",
    "        \"\"\"\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        x = x.squeeze(1)\n",
    "        x = x.permute(2, 0, 1)\n",
    "        # END OF YOUR CODE\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self._reshape_features(x)\n",
    "        \n",
    "        batch_size = x.size(1)\n",
    "        h_0 = self._init_hidden(batch_size)\n",
    "        h_0 = h_0.to(x.device)\n",
    "        x, h = self.rnn(x, h_0)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "adcda60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRNN(Module):\n",
    "    \n",
    "    def __init__(self, alphabet=abc,\n",
    "                 cnn_input_size=(64, 320), cnn_output_len=20,\n",
    "                 rnn_hidden_size=128, rnn_num_layers=2, rnn_dropout=0.3, rnn_bidirectional=False):\n",
    "        super(CRNN, self).__init__()\n",
    "        self.alphabet = alphabet\n",
    "        self.features_extractor = FeatureExtractor(input_size=cnn_input_size, output_len=cnn_output_len)\n",
    "        self.sequence_predictor = SequencePredictor(input_size=self.features_extractor.num_output_features,\n",
    "                                                    hidden_size=rnn_hidden_size, num_layers=rnn_num_layers,\n",
    "                                                    num_classes=len(alphabet)+1, dropout=rnn_dropout,\n",
    "                                                    bidirectional=rnn_bidirectional)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.features_extractor(x)\n",
    "        sequence = self.sequence_predictor(features)\n",
    "        return sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe163ec",
   "metadata": {},
   "source": [
    "Утилиты декодирования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d99c760",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_to_string(pred, abc):\n",
    "    seq = []\n",
    "    for i in range(len(pred)):\n",
    "        label = np.argmax(pred[i])\n",
    "        seq.append(label - 1)\n",
    "    out = []\n",
    "    for i in range(len(seq)):\n",
    "        if len(out) == 0:\n",
    "            if seq[i] != -1:\n",
    "                out.append(seq[i])\n",
    "        else:\n",
    "            if seq[i] != -1 and seq[i] != seq[i - 1]:\n",
    "                out.append(seq[i])\n",
    "    out = ''.join([abc[c] for c in out])\n",
    "    return out\n",
    "\n",
    "def decode(pred, abc):\n",
    "    pred = pred.permute(1, 0, 2).cpu().data.numpy()\n",
    "    outputs = []\n",
    "    for i in range(len(pred)):\n",
    "        outputs.append(pred_to_string(pred[i], abc))\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a8db16c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "crnn = CRNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f79cc0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167\n"
     ]
    }
   ],
   "source": [
    "print(len(abc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4c4b4624",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 3, 64, 320)\n",
    "y = crnn(x)\n",
    "assert y.size() == (20, 1, len(abc) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "81cdc198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 1, 168])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f96ac0e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ЖW(юЯ(Hн}С1xB3ну']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(y, abc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423b4a2d",
   "metadata": {},
   "source": [
    "Параметры обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4a774eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTUALLY_TRAIN = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "27f1805b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "batch_size = 32\n",
    "num_workers = 0\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "crnn.to(device);\n",
    "optimizer = torch.optim.Adam(crnn.parameters(), lr=3e-4, amsgrad=True, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e90ad92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(config)\n",
    "train_size = int(len(config) * 0.95)\n",
    "config_train = config[:train_size // 10]\n",
    "config_val = config[train_size:]\n",
    "\n",
    "train_dataset = RecognitionDataset(config_train, transforms=Resize())\n",
    "val_dataset = RecognitionDataset(config_val, transforms=Resize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4138116d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, \n",
    "                              batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True, \n",
    "                              drop_last=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, \n",
    "                            batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True, \n",
    "                            drop_last=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "523dd13d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 64, 320])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = next(iter(train_dataloader))\n",
    "x['image'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "da86900d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import Levenshtein as lev\n",
    "from torch.nn.functional import ctc_loss, log_softmax\n",
    "\n",
    "class CRNNModule(pl.LightningModule):\n",
    "    def __init__(self, crnn, learning_rate=1e-3):\n",
    "        super().__init__()\n",
    "        self.crnn = crnn\n",
    "        self.learning_rate = learning_rate\n",
    "        self.alphabet = abc\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.crnn(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images = batch[\"image\"]\n",
    "        seqs_gt = batch[\"seq\"]\n",
    "        seq_lens_gt = batch[\"seq_len\"]\n",
    "\n",
    "        seqs_pred = self(images).cpu()\n",
    "        log_probs = log_softmax(seqs_pred, dim=2)\n",
    "        seq_lens_pred = torch.Tensor([seqs_pred.size(0)] * seqs_pred.size(1)).int()\n",
    "        loss = ctc_loss(log_probs=log_probs,\n",
    "                        targets=seqs_gt,\n",
    "                        input_lengths=seq_lens_pred,\n",
    "                        target_lengths=seq_lens_gt,\n",
    "                        zero_infinity=True,)\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=False, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def calculate_cer(self, decoded_preds, targets):\n",
    "        # calculate character error rate (CER)\n",
    "        cers = []\n",
    "        for pred, target in zip(decoded_preds, targets):\n",
    "            cer = lev.distance(pred, target)\n",
    "            cers.append(cer)\n",
    "        avg_cer = sum(cers) / len(cers)\n",
    "        return avg_cer\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images = batch[\"image\"]\n",
    "        seqs_gt = batch[\"seq\"]\n",
    "        seq_lens_gt = batch[\"seq_len\"]\n",
    "        texts = batch[\"text\"]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            seqs_pred = self(images).cpu()\n",
    "        log_probs = log_softmax(seqs_pred, dim=2)\n",
    "        seq_lens_pred = torch.Tensor([seqs_pred.size(0)] * seqs_pred.size(1)).int()\n",
    "        loss = ctc_loss(log_probs=log_probs,\n",
    "                        targets=seqs_gt,\n",
    "                        input_lengths=seq_lens_pred,\n",
    "                        target_lengths=seq_lens_gt,\n",
    "                        zero_infinity=True)\n",
    "\n",
    "        # decode the predictions to text\n",
    "        decoded_preds = decode(seqs_pred, self.alphabet)\n",
    "        cer = self.calculate_cer(decoded_preds, texts)\n",
    "        \n",
    "        self.log('val_loss', loss, on_step=True, on_epoch=False, prog_bar=True)\n",
    "        self.log('val_cer', cer, on_step=True, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8176d113",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "D:\\anacon\\envs\\new\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name | Type | Params\n",
      "------------------------------\n",
      "0 | crnn | CRNN | 11.5 M\n",
      "------------------------------\n",
      "11.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.5 M    Total params\n",
      "46.176    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anacon\\envs\\new\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "D:\\anacon\\envs\\new\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:76: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 32. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n",
      "D:\\anacon\\envs\\new\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2fd4efde2f1461fb76964602bd90377",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anacon\\envs\\new\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:76: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 8. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    }
   ],
   "source": [
    "# crnn и ctc_loss определяются где-то выше...\n",
    "model = CRNNModule(crnn)\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=num_epochs)\n",
    "trainer.fit(model, train_dataloader, val_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0457b1dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 26, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Загрузка изображения\n",
    "path = r\"/Users/vladislavgadzihanov/PycharmProjects/cv_contest/data/vk_made_ocr/train/train/49.jpg\"\n",
    "\n",
    "def rotate_image(image, angle):\n",
    "    (h, w) = image.shape[:2]\n",
    "    center = (w // 2, h // 2)\n",
    "    M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "\n",
    "    # Получаем новые размеры изображения после поворота\n",
    "    cos = np.abs(M[0, 0])\n",
    "    sin = np.abs(M[0, 1])\n",
    "    nW = int((h * sin) + (w * cos))\n",
    "    nH = int((h * cos) + (w * sin))\n",
    "\n",
    "    # Меняем центр изображения для его поворота\n",
    "    M[0, 2] += (nW / 2) - center[0]\n",
    "    M[1, 2] += (nH / 2) - center[1]\n",
    "\n",
    "    # Вычисляем средний цвет краев изображения\n",
    "    border_color = [np.mean(image[0, :], axis=0), np.mean(image[-1, :], axis=0), \n",
    "                    np.mean(image[:, 0], axis=0), np.mean(image[:, -1], axis=0)]\n",
    "    border_color = np.mean(border_color, axis=0).astype(int)\n",
    "\n",
    "    rotated = cv2.warpAffine(image, M, (nW, nH),\n",
    "                             borderMode=cv2.INTER_LINEAR, \n",
    "                             borderValue=tuple(border_color.tolist())\n",
    "                            )\n",
    "    return rotated\n",
    "\n",
    "# Загрузка изображения\n",
    "img = cv2.imread(path)\n",
    "\n",
    "# Задание угла поворота\n",
    "alpha = 90  # Например, угол поворота 45 градусов\n",
    "\n",
    "# Выполнение операции поворота\n",
    "rotated = rotate_image(img, alpha)\n",
    "print(rotated.shape)\n",
    "\n",
    "# Визуализация\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Отображение исходного изображения\n",
    "axs[0].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "axs[0].set_title('Source')\n",
    "axs[0].axis('off')\n",
    "\n",
    "# Отображение повернутого изображения\n",
    "axs[1].imshow(cv2.cvtColor(rotated, cv2.COLOR_BGR2RGB))\n",
    "axs[1].set_title('Rotated')\n",
    "axs[1].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f7fb79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
